{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/stanford-crfm/helm.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePQScR0mxXS",
        "outputId": "c17f99ac-fce2-4a52-8db2-3bb2ccf0feff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/stanford-crfm/helm.git\n",
            "  Cloning https://github.com/stanford-crfm/helm.git to /tmp/pip-req-build-qhmy9n4v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stanford-crfm/helm.git /tmp/pip-req-build-qhmy9n4v\n",
            "  Resolved https://github.com/stanford-crfm/helm.git to commit be3e493f6a7029baab2ed4aaa8a31d992adcd019\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cattrs~=22.2 (from crfm-helm==0.4.0)\n",
            "  Downloading cattrs-22.2.0-py3-none-any.whl (35 kB)\n",
            "Collecting dacite~=1.6 (from crfm-helm==0.4.0)\n",
            "  Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
            "Collecting importlib-resources~=5.10 (from crfm-helm==0.4.0)\n",
            "  Downloading importlib_resources-5.13.0-py3-none-any.whl (32 kB)\n",
            "Collecting Mako~=1.2 (from crfm-helm==0.4.0)\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (1.25.2)\n",
            "Collecting pyhocon~=0.3.59 (from crfm-helm==0.4.0)\n",
            "  Downloading pyhocon-0.3.60.tar.gz (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting retrying~=1.3 (from crfm-helm==0.4.0)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: spacy~=3.5 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (3.7.4)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (4.66.2)\n",
            "Collecting zstandard~=0.18.0 (from crfm-helm==0.4.0)\n",
            "  Downloading zstandard-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict~=1.7 (from crfm-helm==0.4.0)\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bottle~=0.12.23 (from crfm-helm==0.4.0)\n",
            "  Downloading bottle-0.12.25-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.2/90.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets~=2.15 (from crfm-helm==0.4.0)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix~=0.6 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (0.6)\n",
            "Requirement already satisfied: nltk~=3.7 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (3.8.1)\n",
            "Collecting pyext~=0.7 (from crfm-helm==0.4.0)\n",
            "  Downloading pyext-0.7.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rouge-score~=0.1.2 (from crfm-helm==0.4.0)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy~=1.10 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (1.11.4)\n",
            "Collecting uncertainty-calibration~=0.1.4 (from crfm-helm==0.4.0)\n",
            "  Downloading uncertainty-calibration-0.1.4.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn~=1.1 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (1.2.2)\n",
            "Requirement already satisfied: transformers~=4.37 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (4.38.2)\n",
            "Requirement already satisfied: torch<3.0.0,>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision<3.0.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: google-api-python-client~=2.64 in /usr/local/lib/python3.10/dist-packages (from crfm-helm==0.4.0) (2.84.0)\n",
            "Requirement already satisfied: attrs>=20 in /usr/local/lib/python3.10/dist-packages (from cattrs~=22.2->crfm-helm==0.4.0) (23.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from cattrs~=22.2->crfm-helm==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (3.13.3)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets~=2.15->crfm-helm==0.4.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (2.31.0)\n",
            "Collecting xxhash (from datasets~=2.15->crfm-helm==0.4.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets~=2.15->crfm-helm==0.4.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.15->crfm-helm==0.4.0) (6.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.64->crfm-helm==0.4.0) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.64->crfm-helm==0.4.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.64->crfm-helm==0.4.0) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.64->crfm-helm==0.4.0) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client~=2.64->crfm-helm==0.4.0) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako~=1.2->crfm-helm==0.4.0) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->crfm-helm==0.4.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->crfm-helm==0.4.0) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk~=3.7->crfm-helm==0.4.0) (2023.12.25)\n",
            "Requirement already satisfied: pyparsing<4,>=2 in /usr/local/lib/python3.10/dist-packages (from pyhocon~=0.3.59->crfm-helm==0.4.0) (3.1.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying~=1.3->crfm-helm==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score~=0.1.2->crfm-helm==0.4.0) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.1->crfm-helm==0.4.0) (3.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.5->crfm-helm==0.4.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0) (3.2.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.13.1->crfm-helm==0.4.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=1.13.1->crfm-helm==0.4.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<3.0.0,>=0.14.1->crfm-helm==0.4.0) (9.4.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.37->crfm-helm==0.4.0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.37->crfm-helm==0.4.0) (0.4.2)\n",
            "Collecting parameterized (from uncertainty-calibration~=0.1.4->crfm-helm==0.4.0)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.15->crfm-helm==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.15->crfm-helm==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.15->crfm-helm==0.4.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.15->crfm-helm==0.4.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.15->crfm-helm==0.4.0) (4.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.64->crfm-helm==0.4.0) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client~=2.64->crfm-helm==0.4.0) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client~=2.64->crfm-helm==0.4.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client~=2.64->crfm-helm==0.4.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client~=2.64->crfm-helm==0.4.0) (4.9)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.5->crfm-helm==0.4.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.5->crfm-helm==0.4.0) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.15->crfm-helm==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.15->crfm-helm==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.15->crfm-helm==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.15->crfm-helm==0.4.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.5->crfm-helm==0.4.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.5->crfm-helm==0.4.0) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy~=3.5->crfm-helm==0.4.0) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets~=2.15->crfm-helm==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets~=2.15->crfm-helm==0.4.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets~=2.15->crfm-helm==0.4.0) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=1.13.1->crfm-helm==0.4.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client~=2.64->crfm-helm==0.4.0) (0.6.0)\n",
            "Building wheels for collected packages: crfm-helm, pyext, pyhocon, rouge-score, sqlitedict, uncertainty-calibration\n",
            "  Building wheel for crfm-helm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crfm-helm: filename=crfm_helm-0.4.0-py3-none-any.whl size=5592655 sha256=962d0bf7b0848cd62e61d4c126d10446bb43b77f7e3bb84d3ade4db6cd146afc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-510h8ttq/wheels/87/7b/28/c5c6988fef616e6ccfc5146d10e348ac19c94970e7310602ff\n",
            "  Building wheel for pyext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyext: filename=pyext-0.7-py3-none-any.whl size=7222 sha256=2888b4c218c0e5ce95ab7175207227058e94e26725007232dd020624ff15b6de\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/95/a9/f3f15c5e52dec7912c332ae503e82fd680e576bf336437f002\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhocon: filename=pyhocon-0.3.60-py3-none-any.whl size=20865 sha256=edf126528810dd56c323a9e245a20298d0fa99bb7411ac484fdc3298dcdc2fc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/79/47/98ab23fa895daa293e4dd4ce43e581c035ca15e908a2d94104\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=cde2c8b1e8896070fdea34673fd0ce3f088b81aa596c3a408bb0ebb7dd1c6c92\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14390 sha256=358622f0de1934a00cf16ea2b13ac8796afd61ae1f0c21377f071e376718f9aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/8d/5f/8ad5a7e9be1c6774ccc0b832e0020b61e1f9b239491ddffffb\n",
            "  Building wheel for uncertainty-calibration (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uncertainty-calibration: filename=uncertainty_calibration-0.1.4-py3-none-any.whl size=14201 sha256=810c184e622cf1059b8f7c5a55ea28ebd6d667f0c9be68271b32dc96054ef5aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/fb/74/93a4d13a3aa8e85f5f2b570114d0c19c7e1e1366f901c9d1c0\n",
            "Successfully built crfm-helm pyext pyhocon rouge-score sqlitedict uncertainty-calibration\n",
            "Installing collected packages: sqlitedict, pyext, bottle, zstandard, xxhash, retrying, pyhocon, parameterized, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, Mako, importlib-resources, dill, dacite, cattrs, rouge-score, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, uncertainty-calibration, nvidia-cusolver-cu12, datasets, crfm-helm\n",
            "  Attempting uninstall: importlib-resources\n",
            "    Found existing installation: importlib_resources 6.4.0\n",
            "    Uninstalling importlib_resources-6.4.0:\n",
            "      Successfully uninstalled importlib_resources-6.4.0\n",
            "Successfully installed Mako-1.3.2 bottle-0.12.25 cattrs-22.2.0 crfm-helm-0.4.0 dacite-1.8.1 datasets-2.18.0 dill-0.3.8 importlib-resources-5.13.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 parameterized-0.9.0 pyext-0.7 pyhocon-0.3.60 retrying-1.3.4 rouge-score-0.1.2 sqlitedict-1.7.0 uncertainty-calibration-0.1.4 xxhash-3.4.1 zstandard-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "wGudademnImm",
        "outputId": "647307e0-c64e-4b50-ca50-d12fe6721e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-85472a66-dada-4c95-927a-9526f51e7540\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-85472a66-dada-4c95-927a-9526f51e7540\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving run_specs_full_coarse_600_budget.conf to run_specs_full_coarse_600_budget.conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone --recurse-submodules https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwAgObc1psh8",
        "outputId": "ac4026c5-cc63-4cd4-ac78-11a432425f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neurips_llm_efficiency_challenge'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 427 (delta 84), reused 61 (delta 61), pack-reused 302\u001b[K\n",
            "Receiving objects: 100% (427/427), 211.39 KiB | 2.71 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "Submodule 'sample-submissions/lit-gpt/lit-gpt' (https://github.com/Lightning-AI/lit-gpt) registered for path 'sample-submissions/lit-gpt/lit-gpt'\n",
            "Cloning into '/content/neurips_llm_efficiency_challenge/sample-submissions/lit-gpt/lit-gpt'...\n",
            "remote: Enumerating objects: 8648, done.        \n",
            "remote: Counting objects: 100% (568/568), done.        \n",
            "remote: Compressing objects: 100% (330/330), done.        \n",
            "remote: Total 8648 (delta 345), reused 398 (delta 229), pack-reused 8080        \n",
            "Receiving objects: 100% (8648/8648), 3.51 MiB | 14.99 MiB/s, done.\n",
            "Resolving deltas: 100% (6115/6115), done.\n",
            "Submodule path 'sample-submissions/lit-gpt/lit-gpt': checked out 'c195451dfeb073c749df6cc1c5cb2aa654fe9d24'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "sudo apt update -qq\n",
        "\n",
        "sudo apt install apt-transport-https ca-certificates curl software-properties-common -qq\n",
        "\n",
        "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n",
        "\n",
        "sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\n",
        "\n",
        "sudo apt update -qq\n",
        "\n",
        "sudo apt install docker-ce\n",
        "\n",
        "docker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSJV0HM0qDWK",
        "outputId": "0f2ebb1e-77f0-4907-b03c-32f8b75f6f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "ca-certificates is already the newest version (20230311ubuntu0.22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "The following NEW packages will be installed:\n",
            "  apt-transport-https\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,510 B of archives.\n",
            "After this operation, 170 kB of additional disk space will be used.\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package apt-transport-https.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../apt-transport-https_2.4.12_all.deb ...\n",
            "Unpacking apt-transport-https (2.4.12) ...\n",
            "Setting up apt-transport-https (2.4.12) ...\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Repository: 'deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable'\n",
            "Description:\n",
            "Archive for codename: bionic components: stable\n",
            "More info: https://download.docker.com/linux/ubuntu\n",
            "Adding repository.\n",
            "Press [ENTER] to continue or Ctrl-c to cancel.\n",
            "Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_download_docker_com_linux_ubuntu-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_download_docker_com_linux_ubuntu-jammy.list\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages [46.4 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 111 kB in 2s (69.0 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://download.docker.com/linux/ubuntu/dists/bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://download.docker.com/linux/ubuntu/dists/bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor containerd.io docker-buildx-plugin docker-ce-cli\n",
            "  docker-ce-rootless-extras docker-compose-plugin iptables libip6tc2\n",
            "  libnetfilter-conntrack3 libnfnetlink0 libnftnl11 libslirp0 netbase pigz\n",
            "  slirp4netns\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils aufs-tools cgroupfs-mount\n",
            "  | cgroup-lite firewalld nftables\n",
            "The following NEW packages will be installed:\n",
            "  apparmor containerd.io docker-buildx-plugin docker-ce docker-ce-cli\n",
            "  docker-ce-rootless-extras docker-compose-plugin iptables libip6tc2\n",
            "  libnetfilter-conntrack3 libnfnetlink0 libnftnl11 libslirp0 netbase pigz\n",
            "  slirp4netns\n",
            "0 upgraded, 16 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 112 MB of archives.\n",
            "After this operation, 407 MB of additional disk space will be used.\n",
            "Get:1 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.6.21-1 [28.3 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pigz amd64 2.6-1 [63.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.3 [595 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libip6tc2 amd64 1.8.7-1ubuntu5.2 [20.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnfnetlink0 amd64 1.0.1-3build3 [14.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnetfilter-conntrack3 amd64 1.0.9-1 [45.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnftnl11 amd64 1.2.1-1build1 [65.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 iptables amd64 1.8.7-1ubuntu5.2 [455 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libslirp0 amd64 4.6.1-1build1 [61.5 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 slirp4netns amd64 1.0.1-2 [28.2 kB]\n",
            "Get:12 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-buildx-plugin amd64 0.10.5-1~ubuntu.18.04~bionic [26.1 MB]\n",
            "Get:13 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:24.0.2-1~ubuntu.18.04~bionic [13.3 MB]\n",
            "Get:14 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:24.0.2-1~ubuntu.18.04~bionic [22.9 MB]\n",
            "Get:15 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-rootless-extras amd64 5:24.0.2-1~ubuntu.18.04~bionic [9,014 kB]\n",
            "Get:16 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-compose-plugin amd64 2.18.1-1~ubuntu.18.04~bionic [10.9 MB]\n",
            "Fetched 112 MB in 2s (62.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package pigz.\n",
            "(Reading database ... 121757 files and directories currently installed.)\n",
            "Preparing to unpack .../00-pigz_2.6-1_amd64.deb ...\n",
            "Unpacking pigz (2.6-1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package apparmor.\n",
            "Preparing to unpack .../02-apparmor_3.0.4-2ubuntu2.3_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.3) ...\n",
            "Selecting previously unselected package libip6tc2:amd64.\n",
            "Preparing to unpack .../03-libip6tc2_1.8.7-1ubuntu5.2_amd64.deb ...\n",
            "Unpacking libip6tc2:amd64 (1.8.7-1ubuntu5.2) ...\n",
            "Selecting previously unselected package libnfnetlink0:amd64.\n",
            "Preparing to unpack .../04-libnfnetlink0_1.0.1-3build3_amd64.deb ...\n",
            "Unpacking libnfnetlink0:amd64 (1.0.1-3build3) ...\n",
            "Selecting previously unselected package libnetfilter-conntrack3:amd64.\n",
            "Preparing to unpack .../05-libnetfilter-conntrack3_1.0.9-1_amd64.deb ...\n",
            "Unpacking libnetfilter-conntrack3:amd64 (1.0.9-1) ...\n",
            "Selecting previously unselected package libnftnl11:amd64.\n",
            "Preparing to unpack .../06-libnftnl11_1.2.1-1build1_amd64.deb ...\n",
            "Unpacking libnftnl11:amd64 (1.2.1-1build1) ...\n",
            "Selecting previously unselected package iptables.\n",
            "Preparing to unpack .../07-iptables_1.8.7-1ubuntu5.2_amd64.deb ...\n",
            "Unpacking iptables (1.8.7-1ubuntu5.2) ...\n",
            "Selecting previously unselected package containerd.io.\n",
            "Preparing to unpack .../08-containerd.io_1.6.21-1_amd64.deb ...\n",
            "Unpacking containerd.io (1.6.21-1) ...\n",
            "Selecting previously unselected package docker-buildx-plugin.\n",
            "Preparing to unpack .../09-docker-buildx-plugin_0.10.5-1~ubuntu.18.04~bionic_amd64.deb ...\n",
            "Unpacking docker-buildx-plugin (0.10.5-1~ubuntu.18.04~bionic) ...\n",
            "Selecting previously unselected package docker-ce-cli.\n",
            "Preparing to unpack .../10-docker-ce-cli_5%3a24.0.2-1~ubuntu.18.04~bionic_amd64.deb ...\n",
            "Unpacking docker-ce-cli (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Selecting previously unselected package docker-ce.\n",
            "Preparing to unpack .../11-docker-ce_5%3a24.0.2-1~ubuntu.18.04~bionic_amd64.deb ...\n",
            "Unpacking docker-ce (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Selecting previously unselected package docker-ce-rootless-extras.\n",
            "Preparing to unpack .../12-docker-ce-rootless-extras_5%3a24.0.2-1~ubuntu.18.04~bionic_amd64.deb ...\n",
            "Unpacking docker-ce-rootless-extras (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Selecting previously unselected package docker-compose-plugin.\n",
            "Preparing to unpack .../13-docker-compose-plugin_2.18.1-1~ubuntu.18.04~bionic_amd64.deb ...\n",
            "Unpacking docker-compose-plugin (2.18.1-1~ubuntu.18.04~bionic) ...\n",
            "Selecting previously unselected package libslirp0:amd64.\n",
            "Preparing to unpack .../14-libslirp0_4.6.1-1build1_amd64.deb ...\n",
            "Unpacking libslirp0:amd64 (4.6.1-1build1) ...\n",
            "Selecting previously unselected package slirp4netns.\n",
            "Preparing to unpack .../15-slirp4netns_1.0.1-2_amd64.deb ...\n",
            "Unpacking slirp4netns (1.0.1-2) ...\n",
            "Setting up libip6tc2:amd64 (1.8.7-1ubuntu5.2) ...\n",
            "Setting up libnftnl11:amd64 (1.2.1-1build1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.3) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up docker-buildx-plugin (0.10.5-1~ubuntu.18.04~bionic) ...\n",
            "Setting up containerd.io (1.6.21-1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.\n",
            "Setting up docker-compose-plugin (2.18.1-1~ubuntu.18.04~bionic) ...\n",
            "Setting up docker-ce-cli (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Setting up libslirp0:amd64 (4.6.1-1build1) ...\n",
            "Setting up pigz (2.6-1) ...\n",
            "Setting up libnfnetlink0:amd64 (1.0.1-3build3) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up docker-ce-rootless-extras (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Setting up slirp4netns (1.0.1-2) ...\n",
            "Setting up libnetfilter-conntrack3:amd64 (1.0.9-1) ...\n",
            "Setting up iptables (1.8.7-1ubuntu5.2) ...\n",
            "update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ip6tables-legacy to provide /usr/sbin/ip6tables (ip6tables) in auto mode\n",
            "update-alternatives: using /usr/sbin/iptables-nft to provide /usr/sbin/iptables (iptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ip6tables-nft to provide /usr/sbin/ip6tables (ip6tables) in auto mode\n",
            "update-alternatives: using /usr/sbin/arptables-nft to provide /usr/sbin/arptables (arptables) in auto mode\n",
            "update-alternatives: using /usr/sbin/ebtables-nft to provide /usr/sbin/ebtables (ebtables) in auto mode\n",
            "Setting up docker-ce (5:24.0.2-1~ubuntu.18.04~bionic) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /lib/systemd/system/docker.service.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/docker.socket → /lib/systemd/system/docker.socket.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "\n",
            "Usage:  docker [OPTIONS] COMMAND\n",
            "\n",
            "A self-sufficient runtime for containers\n",
            "\n",
            "Common Commands:\n",
            "  run         Create and run a new container from an image\n",
            "  exec        Execute a command in a running container\n",
            "  ps          List containers\n",
            "  build       Build an image from a Dockerfile\n",
            "  pull        Download an image from a registry\n",
            "  push        Upload an image to a registry\n",
            "  images      List images\n",
            "  login       Log in to a registry\n",
            "  logout      Log out from a registry\n",
            "  search      Search Docker Hub for images\n",
            "  version     Show the Docker version information\n",
            "  info        Display system-wide information\n",
            "\n",
            "Management Commands:\n",
            "  builder     Manage builds\n",
            "  buildx*     Docker Buildx (Docker Inc., v0.10.5)\n",
            "  checkpoint  Manage checkpoints\n",
            "  compose*    Docker Compose (Docker Inc., v2.18.1)\n",
            "  container   Manage containers\n",
            "  context     Manage contexts\n",
            "  image       Manage images\n",
            "  manifest    Manage Docker image manifests and manifest lists\n",
            "  network     Manage networks\n",
            "  plugin      Manage plugins\n",
            "  system      Manage Docker\n",
            "  trust       Manage trust on Docker images\n",
            "  volume      Manage volumes\n",
            "\n",
            "Swarm Commands:\n",
            "  config      Manage Swarm configs\n",
            "  node        Manage Swarm nodes\n",
            "  secret      Manage Swarm secrets\n",
            "  service     Manage Swarm services\n",
            "  stack       Manage Swarm stacks\n",
            "  swarm       Manage Swarm\n",
            "\n",
            "Commands:\n",
            "  attach      Attach local standard input, output, and error streams to a running container\n",
            "  commit      Create a new image from a container's changes\n",
            "  cp          Copy files/folders between a container and the local filesystem\n",
            "  create      Create a new container\n",
            "  diff        Inspect changes to files or directories on a container's filesystem\n",
            "  events      Get real time events from the server\n",
            "  export      Export a container's filesystem as a tar archive\n",
            "  history     Show the history of an image\n",
            "  import      Import the contents from a tarball to create a filesystem image\n",
            "  inspect     Return low-level information on Docker objects\n",
            "  kill        Kill one or more running containers\n",
            "  load        Load an image from a tar archive or STDIN\n",
            "  logs        Fetch the logs of a container\n",
            "  pause       Pause all processes within one or more containers\n",
            "  port        List port mappings or a specific mapping for the container\n",
            "  rename      Rename a container\n",
            "  restart     Restart one or more containers\n",
            "  rm          Remove one or more containers\n",
            "  rmi         Remove one or more images\n",
            "  save        Save one or more images to a tar archive (streamed to STDOUT by default)\n",
            "  start       Start one or more stopped containers\n",
            "  stats       Display a live stream of container(s) resource usage statistics\n",
            "  stop        Stop one or more running containers\n",
            "  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n",
            "  top         Display the running processes of a container\n",
            "  unpause     Unpause all processes within one or more containers\n",
            "  update      Update configuration of one or more containers\n",
            "  wait        Block until one or more containers stop, then print their exit codes\n",
            "\n",
            "Global Options:\n",
            "      --config string      Location of client config files (default \"/root/.docker\")\n",
            "  -c, --context string     Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with \"docker context use\")\n",
            "  -D, --debug              Enable debug mode\n",
            "  -H, --host list          Daemon socket to connect to\n",
            "  -l, --log-level string   Set the logging level (\"debug\", \"info\", \"warn\", \"error\", \"fatal\") (default \"info\")\n",
            "      --tls                Use TLS; implied by --tlsverify\n",
            "      --tlscacert string   Trust certs signed only by this CA (default \"/root/.docker/ca.pem\")\n",
            "      --tlscert string     Path to TLS certificate file (default \"/root/.docker/cert.pem\")\n",
            "      --tlskey string      Path to TLS key file (default \"/root/.docker/key.pem\")\n",
            "      --tlsverify          Use TLS and verify the remote\n",
            "  -v, --version            Print version information and quit\n",
            "\n",
            "Run 'docker COMMAND --help' for more information on a command.\n",
            "\n",
            "For more help on how to use Docker, head to https://docs.docker.com/go/guides/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! docker build -t sample_submission ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkG_TOm7p6Qf",
        "outputId": "486ab557-c376-4bf2-80af-ecef6d9fb15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! docker run --gpus all -p 8080:80 sample_submission\n"
      ],
      "metadata": {
        "id": "MwOK_rOctME4",
        "outputId": "2f3846db-57f0-411d-c219-ecce54647e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.\n",
            "See 'docker run --help'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! helm-run --conf-paths run_specs_full_coarse_600_budget.conf --suite v1 --max-eval-instances 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvfqAVTtnVhK",
        "outputId": "6bf1e6e8-d814-476f-da35-89f3fdbc77bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main {\n",
            "  Reading tokenizer configs from /usr/local/lib/python3.10/dist-packages/helm/config/tokenizer_configs.yaml...\n",
            "  Reading model deployments from /usr/local/lib/python3.10/dist-packages/helm/config/model_deployments.yaml...\n",
            "  Read 88 run entries from run_specs_full_coarse_600_budget.conf\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/auto_debugging/task.json -O benchmark_output/scenarios/big_bench/auto_debugging/task.json.tmp\n",
            "--2024-04-05 19:02:36--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/auto_debugging/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6655 (6.5K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/auto_debugging/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   6.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:36 (53.8 MB/s) - ‘benchmark_output/scenarios/big_bench/auto_debugging/task.json.tmp’ saved [6655/6655]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/auto_debugging/task.json.tmp benchmark_output/scenarios/big_bench/auto_debugging/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/auto_debugging/task.json to benchmark_output/scenarios/big_bench/auto_debugging/task.json\n",
            "  } [0.188s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/code_line_description/task.json -O benchmark_output/scenarios/big_bench/code_line_description/task.json.tmp\n",
            "--2024-04-05 19:02:36--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/code_line_description/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22646 (22K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/code_line_description/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  22.12K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-04-05 19:02:36 (13.4 MB/s) - ‘benchmark_output/scenarios/big_bench/code_line_description/task.json.tmp’ saved [22646/22646]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/code_line_description/task.json.tmp benchmark_output/scenarios/big_bench/code_line_description/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/code_line_description/task.json to benchmark_output/scenarios/big_bench/code_line_description/task.json\n",
            "  } [0.2s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/contradictions/task.json -O benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json.tmp\n",
            "--2024-04-05 19:02:36--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/contradictions/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7325 (7.2K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   7.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:36 (35.0 MB/s) - ‘benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json.tmp’ saved [7325/7325]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json.tmp benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/contradictions/task.json to benchmark_output/scenarios/big_bench/conceptual_combinations/contradictions/task.json\n",
            "  } [0.159s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/emergent_properties/task.json -O benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json.tmp\n",
            "--2024-04-05 19:02:36--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/emergent_properties/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16779 (16K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  16.39K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-04-05 19:02:36 (21.4 MB/s) - ‘benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json.tmp’ saved [16779/16779]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json.tmp benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/emergent_properties/task.json to benchmark_output/scenarios/big_bench/conceptual_combinations/emergent_properties/task.json\n",
            "  } [0.176s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/fanciful_fictional_combinations/task.json -O benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json.tmp\n",
            "--2024-04-05 19:02:36--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/fanciful_fictional_combinations/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5842 (5.7K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   5.71K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:37 (44.0 MB/s) - ‘benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json.tmp’ saved [5842/5842]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json.tmp benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/fanciful_fictional_combinations/task.json to benchmark_output/scenarios/big_bench/conceptual_combinations/fanciful_fictional_combinations/task.json\n",
            "  } [0.17s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/homonyms/task.json -O benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json.tmp\n",
            "--2024-04-05 19:02:37--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/homonyms/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4480 (4.4K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   4.38K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:37 (23.4 MB/s) - ‘benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json.tmp’ saved [4480/4480]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json.tmp benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/homonyms/task.json to benchmark_output/scenarios/big_bench/conceptual_combinations/homonyms/task.json\n",
            "  } [0.163s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/invented_words/task.json -O benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json.tmp\n",
            "--2024-04-05 19:02:37--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/invented_words/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7465 (7.3K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   7.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:37 (37.2 MB/s) - ‘benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json.tmp’ saved [7465/7465]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json.tmp benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/conceptual_combinations/invented_words/task.json to benchmark_output/scenarios/big_bench/conceptual_combinations/invented_words/task.json\n",
            "  } [0.161s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/emoji_movie/task.json -O benchmark_output/scenarios/big_bench/emoji_movie/task.json.tmp\n",
            "--2024-04-05 19:02:37--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/emoji_movie/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30457 (30K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/emoji_movie/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  29.74K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-04-05 19:02:37 (10.5 MB/s) - ‘benchmark_output/scenarios/big_bench/emoji_movie/task.json.tmp’ saved [30457/30457]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/emoji_movie/task.json.tmp benchmark_output/scenarios/big_bench/emoji_movie/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/emoji_movie/task.json to benchmark_output/scenarios/big_bench/emoji_movie/task.json\n",
            "  } [0.153s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/formal_fallacies_syllogisms_negation/task.json -O benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json.tmp\n",
            "--2024-04-05 19:02:37--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/formal_fallacies_syllogisms_negation/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9569470 (9.1M) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   9.13M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-04-05 19:02:37 (84.5 MB/s) - ‘benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json.tmp’ saved [9569470/9569470]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json.tmp benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/formal_fallacies_syllogisms_negation/task.json to benchmark_output/scenarios/big_bench/formal_fallacies_syllogisms_negation/task.json\n",
            "  } [0.342s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/known_unknowns/task.json -O benchmark_output/scenarios/big_bench/known_unknowns/task.json.tmp\n",
            "--2024-04-05 19:02:38--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/known_unknowns/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8265 (8.1K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/known_unknowns/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   8.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:38 (78.4 MB/s) - ‘benchmark_output/scenarios/big_bench/known_unknowns/task.json.tmp’ saved [8265/8265]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/known_unknowns/task.json.tmp benchmark_output/scenarios/big_bench/known_unknowns/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/known_unknowns/task.json to benchmark_output/scenarios/big_bench/known_unknowns/task.json\n",
            "  } [0.225s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/linguistics_puzzles/task.json -O benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json.tmp\n",
            "--2024-04-05 19:02:38--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/linguistics_puzzles/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1812592 (1.7M) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   1.73M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-05 19:02:38 (27.6 MB/s) - ‘benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json.tmp’ saved [1812592/1812592]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json.tmp benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/linguistics_puzzles/task.json to benchmark_output/scenarios/big_bench/linguistics_puzzles/task.json\n",
            "  } [0.37s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logic_grid_puzzle/task.json -O benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json.tmp\n",
            "--2024-04-05 19:02:38--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logic_grid_puzzle/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1542085 (1.5M) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   1.47M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-05 19:02:39 (24.0 MB/s) - ‘benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json.tmp’ saved [1542085/1542085]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json.tmp benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logic_grid_puzzle/task.json to benchmark_output/scenarios/big_bench/logic_grid_puzzle/task.json\n",
            "  } [0.314s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/three_objects/task.json -O benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json.tmp\n",
            "--2024-04-05 19:02:39--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/three_objects/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120606 (118K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 117.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:39 (4.93 MB/s) - ‘benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json.tmp’ saved [120606/120606]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json.tmp benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/three_objects/task.json to benchmark_output/scenarios/big_bench/logical_deduction/three_objects/task.json\n",
            "  } [0.222s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/five_objects/task.json -O benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json.tmp\n",
            "--2024-04-05 19:02:39--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/five_objects/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305889 (299K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 298.72K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-05 19:02:39 (7.63 MB/s) - ‘benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json.tmp’ saved [305889/305889]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json.tmp benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/five_objects/task.json to benchmark_output/scenarios/big_bench/logical_deduction/five_objects/task.json\n",
            "  } [0.212s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json -O benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json.tmp\n",
            "--2024-04-05 19:02:39--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 579048 (565K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 565.48K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-05 19:02:39 (12.6 MB/s) - ‘benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json.tmp’ saved [579048/579048]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json.tmp benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json to benchmark_output/scenarios/big_bench/logical_deduction/seven_objects/task.json\n",
            "  } [0.24s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/novel_concepts/task.json -O benchmark_output/scenarios/big_bench/novel_concepts/task.json.tmp\n",
            "--2024-04-05 19:02:39--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/novel_concepts/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/novel_concepts/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:39 (51.4 MB/s) - ‘benchmark_output/scenarios/big_bench/novel_concepts/task.json.tmp’ saved [12257/12257]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/novel_concepts/task.json.tmp benchmark_output/scenarios/big_bench/novel_concepts/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/novel_concepts/task.json to benchmark_output/scenarios/big_bench/novel_concepts/task.json\n",
            "  } [0.149s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/operators/task.json -O benchmark_output/scenarios/big_bench/operators/task.json.tmp\n",
            "--2024-04-05 19:02:39--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/operators/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25831 (25K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/operators/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  25.23K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-04-05 19:02:40 (12.2 MB/s) - ‘benchmark_output/scenarios/big_bench/operators/task.json.tmp’ saved [25831/25831]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/operators/task.json.tmp benchmark_output/scenarios/big_bench/operators/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/operators/task.json to benchmark_output/scenarios/big_bench/operators/task.json\n",
            "  } [0.16s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/play_dialog_same_or_different/task.json -O benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json.tmp\n",
            "--2024-04-05 19:02:40--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/play_dialog_same_or_different/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2548922 (2.4M) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   2.43M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-04-05 19:02:40 (34.8 MB/s) - ‘benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json.tmp’ saved [2548922/2548922]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json.tmp benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/play_dialog_same_or_different/task.json to benchmark_output/scenarios/big_bench/play_dialog_same_or_different/task.json\n",
            "  } [0.437s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/repeat_copy_logic/task.json -O benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json.tmp\n",
            "--2024-04-05 19:02:40--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/repeat_copy_logic/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8103 (7.9K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]   7.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 19:02:40 (70.7 MB/s) - ‘benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json.tmp’ saved [8103/8103]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json.tmp benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/repeat_copy_logic/task.json to benchmark_output/scenarios/big_bench/repeat_copy_logic/task.json\n",
            "  } [0.145s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/boolean/task.json -O benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json.tmp\n",
            "--2024-04-05 19:02:40--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/boolean/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26855 (26K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  26.23K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2024-04-05 19:02:40 (16.8 MB/s) - ‘benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json.tmp’ saved [26855/26855]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json.tmp benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/boolean/task.json to benchmark_output/scenarios/big_bench/strange_stories/boolean/task.json\n",
            "  } [0.169s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/multiple_choice/task.json -O benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json.tmp\n",
            "--2024-04-05 19:02:40--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/multiple_choice/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94461 (92K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  92.25K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:41 (5.36 MB/s) - ‘benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json.tmp’ saved [94461/94461]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json.tmp benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strange_stories/multiple_choice/task.json to benchmark_output/scenarios/big_bench/strange_stories/multiple_choice/task.json\n",
            "  } [0.162s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strategyqa/task.json -O benchmark_output/scenarios/big_bench/strategyqa/task.json.tmp\n",
            "--2024-04-05 19:02:41--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strategyqa/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 854248 (834K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/strategyqa/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 834.23K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-05 19:02:41 (16.9 MB/s) - ‘benchmark_output/scenarios/big_bench/strategyqa/task.json.tmp’ saved [854248/854248]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/strategyqa/task.json.tmp benchmark_output/scenarios/big_bench/strategyqa/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/strategyqa/task.json to benchmark_output/scenarios/big_bench/strategyqa/task.json\n",
            "  } [0.271s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/adversarial/task.json -O benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json.tmp\n",
            "--2024-04-05 19:02:41--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/adversarial/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 124352 (121K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 121.44K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:41 (4.93 MB/s) - ‘benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json.tmp’ saved [124352/124352]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json.tmp benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/adversarial/task.json to benchmark_output/scenarios/big_bench/symbol_interpretation/adversarial/task.json\n",
            "  } [0.187s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/emoji_agnostic/task.json -O benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json.tmp\n",
            "--2024-04-05 19:02:41--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/emoji_agnostic/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127758 (125K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 124.76K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:41 (5.11 MB/s) - ‘benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json.tmp’ saved [127758/127758]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json.tmp benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/emoji_agnostic/task.json to benchmark_output/scenarios/big_bench/symbol_interpretation/emoji_agnostic/task.json\n",
            "  } [0.194s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/name_agnostic/task.json -O benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json.tmp\n",
            "--2024-04-05 19:02:41--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/name_agnostic/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 110777 (108K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 108.18K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:41 (4.36 MB/s) - ‘benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json.tmp’ saved [110777/110777]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json.tmp benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/name_agnostic/task.json to benchmark_output/scenarios/big_bench/symbol_interpretation/name_agnostic/task.json\n",
            "  } [0.19s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/plain/task.json -O benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json.tmp\n",
            "--2024-04-05 19:02:41--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/plain/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 125266 (122K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 122.33K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:42 (5.23 MB/s) - ‘benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json.tmp’ saved [125266/125266]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json.tmp benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/plain/task.json to benchmark_output/scenarios/big_bench/symbol_interpretation/plain/task.json\n",
            "  } [0.175s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/tricky/task.json -O benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json.tmp\n",
            "--2024-04-05 19:02:42--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/tricky/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 125267 (122K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 122.33K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-05 19:02:42 (5.14 MB/s) - ‘benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json.tmp’ saved [125267/125267]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json.tmp benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/symbol_interpretation/tricky/task.json to benchmark_output/scenarios/big_bench/symbol_interpretation/tricky/task.json\n",
            "  } [0.185s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/vitaminc_fact_verification/task.json -O benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json.tmp\n",
            "--2024-04-05 19:02:42--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/vitaminc_fact_verification/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23315055 (22M) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>]  22.23M  89.2MB/s    in 0.2s    \n",
            "\n",
            "2024-04-05 19:02:42 (89.2 MB/s) - ‘benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json.tmp’ saved [23315055/23315055]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json.tmp benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/vitaminc_fact_verification/task.json to benchmark_output/scenarios/big_bench/vitaminc_fact_verification/task.json\n",
            "  } [0.628s]\n",
            "  ensure_file_downloaded {\n",
            "    Executing: wget https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/winowhy/task.json -O benchmark_output/scenarios/big_bench/winowhy/task.json.tmp\n",
            "--2024-04-05 19:02:43--  https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/winowhy/task.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 703346 (687K) [text/plain]\n",
            "Saving to: ‘benchmark_output/scenarios/big_bench/winowhy/task.json.tmp’\n",
            "\n",
            "benchmark_output/sc 100%[===================>] 686.86K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-05 19:02:43 (14.4 MB/s) - ‘benchmark_output/scenarios/big_bench/winowhy/task.json.tmp’ saved [703346/703346]\n",
            "\n",
            "    Executing: mv benchmark_output/scenarios/big_bench/winowhy/task.json.tmp benchmark_output/scenarios/big_bench/winowhy/task.json\n",
            "    Finished downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/winowhy/task.json to benchmark_output/scenarios/big_bench/winowhy/task.json\n",
            "  } [0.277s]\n",
            "  88 entries produced 175 run specs\n",
            "  run_specs {\n",
            "    RunSpec(name='big_bench:task=auto_debugging,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'auto_debugging', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_auto_debugging'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=auto_debugging,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'auto_debugging', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_auto_debugging'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=auto_debugging,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'auto_debugging', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_auto_debugging'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=auto_debugging,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'auto_debugging', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_auto_debugging'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=code_line_description,model=neurips_local,max_train_instances=0,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'code_line_description', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\nPython code:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n\\nEnglish language description:\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_code_line_description'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=code_line_description,model=neurips_local,max_train_instances=1,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'code_line_description', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\nPython code:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n\\nEnglish language description:\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_code_line_description'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=code_line_description,model=neurips_local,max_train_instances=2,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'code_line_description', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\nPython code:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n\\nEnglish language description:\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_code_line_description'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=code_line_description,model=neurips_local,max_train_instances=3,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'code_line_description', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\nPython code:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\n\\nEnglish language description:\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_code_line_description'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=contradictions,model=neurips_local,max_train_instances=0,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'contradictions'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=contradictions,model=neurips_local,max_train_instances=1,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'contradictions'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=contradictions,model=neurips_local,max_train_instances=2,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'contradictions'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=contradictions,model=neurips_local,max_train_instances=3,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'contradictions'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=emergent_properties,model=neurips_local,max_train_instances=0,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'emergent_properties'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=emergent_properties,model=neurips_local,max_train_instances=1,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'emergent_properties'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=emergent_properties,model=neurips_local,max_train_instances=2,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'emergent_properties'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=emergent_properties,model=neurips_local,max_train_instances=3,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'emergent_properties'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=fanciful_fictional_combinations,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'fanciful_fictional_combinations'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=fanciful_fictional_combinations,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'fanciful_fictional_combinations'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=fanciful_fictional_combinations,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'fanciful_fictional_combinations'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=fanciful_fictional_combinations,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'fanciful_fictional_combinations'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=homonyms,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'homonyms'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=homonyms,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'homonyms'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=homonyms,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'homonyms'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=homonyms,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'homonyms'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=invented_words,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'invented_words'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=invented_words,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'invented_words'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=invented_words,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'invented_words'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=conceptual_combinations,subtask=invented_words,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'conceptual_combinations', 'subtask': 'invented_words'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_conceptual_combinations'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=emoji_movie,model=neurips_local,max_train_instances=0,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'emoji_movie', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_emoji_movie'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=emoji_movie,model=neurips_local,max_train_instances=1,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'emoji_movie', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_emoji_movie'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=emoji_movie,model=neurips_local,max_train_instances=2,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'emoji_movie', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_emoji_movie'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=emoji_movie,model=neurips_local,max_train_instances=3,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'emoji_movie', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_emoji_movie'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=formal_fallacies_syllogisms_negation,model=neurips_local,max_train_instances=0,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'formal_fallacies_syllogisms_negation', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_formal_fallacies_syllogisms_negation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=formal_fallacies_syllogisms_negation,model=neurips_local,max_train_instances=1,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'formal_fallacies_syllogisms_negation', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_formal_fallacies_syllogisms_negation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=formal_fallacies_syllogisms_negation,model=neurips_local,max_train_instances=2,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'formal_fallacies_syllogisms_negation', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_formal_fallacies_syllogisms_negation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=formal_fallacies_syllogisms_negation,model=neurips_local,max_train_instances=3,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'formal_fallacies_syllogisms_negation', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_formal_fallacies_syllogisms_negation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=known_unknowns,model=neurips_local,max_train_instances=0,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'known_unknowns', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_known_unknowns'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=known_unknowns,model=neurips_local,max_train_instances=1,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'known_unknowns', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_known_unknowns'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=known_unknowns,model=neurips_local,max_train_instances=2,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'known_unknowns', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_known_unknowns'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=known_unknowns,model=neurips_local,max_train_instances=3,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'known_unknowns', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_known_unknowns'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=linguistics_puzzles,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'linguistics_puzzles', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['bleu_4', 'bleu_1', 'rouge_2', 'exact_match', 'rouge_1', 'rouge_l', 'quasi_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_linguistics_puzzles'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=linguistics_puzzles,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'linguistics_puzzles', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['bleu_4', 'bleu_1', 'rouge_2', 'exact_match', 'rouge_1', 'rouge_l', 'quasi_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_linguistics_puzzles'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=linguistics_puzzles,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'linguistics_puzzles', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['bleu_4', 'bleu_1', 'rouge_2', 'exact_match', 'rouge_1', 'rouge_l', 'quasi_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_linguistics_puzzles'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=linguistics_puzzles,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'linguistics_puzzles', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['bleu_4', 'bleu_1', 'rouge_2', 'exact_match', 'rouge_1', 'rouge_l', 'quasi_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_linguistics_puzzles'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logic_grid_puzzle,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logic_grid_puzzle', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logic_grid_puzzle'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logic_grid_puzzle,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logic_grid_puzzle', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logic_grid_puzzle'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logic_grid_puzzle,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logic_grid_puzzle', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logic_grid_puzzle'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logic_grid_puzzle,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logic_grid_puzzle', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logic_grid_puzzle'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=three_objects,model=neurips_local,max_train_instances=0,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'three_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=three_objects,model=neurips_local,max_train_instances=1,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'three_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=three_objects,model=neurips_local,max_train_instances=2,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'three_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=three_objects,model=neurips_local,max_train_instances=3,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'three_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=five_objects,model=neurips_local,max_train_instances=0,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'five_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=five_objects,model=neurips_local,max_train_instances=1,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'five_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=five_objects,model=neurips_local,max_train_instances=2,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'five_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=five_objects,model=neurips_local,max_train_instances=3,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'five_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=seven_objects,model=neurips_local,max_train_instances=0,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'seven_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=seven_objects,model=neurips_local,max_train_instances=1,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'seven_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=seven_objects,model=neurips_local,max_train_instances=2,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'seven_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=logical_deduction,subtask=seven_objects,model=neurips_local,max_train_instances=3,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'logical_deduction', 'subtask': 'seven_objects'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=6, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_logical_deduction'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=novel_concepts,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'novel_concepts', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix=\"Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\\n\", input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_novel_concepts'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=novel_concepts,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'novel_concepts', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix=\"Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\\n\", input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_novel_concepts'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=novel_concepts,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'novel_concepts', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix=\"Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\\n\", input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_novel_concepts'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=novel_concepts,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'novel_concepts', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix=\"Let's do some find-the-common-concept problems. In these problems, your goal is to identify the underlying concept or theme that relates the things listed. Make sure to answer carefully.\\n\", input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_novel_concepts'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=operators,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'operators', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Given the definition of the op operator, compute the result.', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' = ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_operators'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=operators,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'operators', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Given the definition of the op operator, compute the result.', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' = ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_operators'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=operators,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'operators', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Given the definition of the op operator, compute the result.', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' = ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_operators'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=operators,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'operators', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='Given the definition of the op operator, compute the result.', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix=' = ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_operators'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=play_dialog_same_or_different,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'play_dialog_same_or_different', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nThe following transcripts of dialogues have been taken from Shakespeare plays, but the transcripts do not say who said what.  Your task is to identify whether the sentences in question were spoken by the same or different people.', input_prefix='\\nDialogue:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_play_dialog_same_or_different'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=play_dialog_same_or_different,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'play_dialog_same_or_different', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nThe following transcripts of dialogues have been taken from Shakespeare plays, but the transcripts do not say who said what.  Your task is to identify whether the sentences in question were spoken by the same or different people.', input_prefix='\\nDialogue:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_play_dialog_same_or_different'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=play_dialog_same_or_different,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'play_dialog_same_or_different', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nThe following transcripts of dialogues have been taken from Shakespeare plays, but the transcripts do not say who said what.  Your task is to identify whether the sentences in question were spoken by the same or different people.', input_prefix='\\nDialogue:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_play_dialog_same_or_different'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=play_dialog_same_or_different,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'play_dialog_same_or_different', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nThe following transcripts of dialogues have been taken from Shakespeare plays, but the transcripts do not say who said what.  Your task is to identify whether the sentences in question were spoken by the same or different people.', input_prefix='\\nDialogue:\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nAnswer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_play_dialog_same_or_different'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=repeat_copy_logic,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'repeat_copy_logic', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='repeat with logic:\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_repeat_copy_logic'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=repeat_copy_logic,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'repeat_copy_logic', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='repeat with logic:\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_repeat_copy_logic'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=repeat_copy_logic,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'repeat_copy_logic', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='repeat with logic:\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_repeat_copy_logic'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=repeat_copy_logic,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'repeat_copy_logic', 'subtask': ''}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='repeat with logic:\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['quasi_exact_match', 'exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_repeat_copy_logic'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=boolean,model=neurips_local,max_train_instances=0,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'boolean'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=boolean,model=neurips_local,max_train_instances=1,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'boolean'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=boolean,model=neurips_local,max_train_instances=2,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'boolean'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=boolean,model=neurips_local,max_train_instances=3,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'boolean'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=multiple_choice,model=neurips_local,max_train_instances=0,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'multiple_choice'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=multiple_choice,model=neurips_local,max_train_instances=1,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'multiple_choice'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=multiple_choice,model=neurips_local,max_train_instances=2,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'multiple_choice'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strange_stories,subtask=multiple_choice,model=neurips_local,max_train_instances=3,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strange_stories', 'subtask': 'multiple_choice'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Context: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nA: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strange_stories'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strategyqa,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strategyqa', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strategyqa'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strategyqa,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strategyqa', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strategyqa'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strategyqa,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strategyqa', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strategyqa'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=strategyqa,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'strategyqa', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_strategyqa'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=adversarial,model=neurips_local,max_train_instances=0,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'adversarial'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔺 is a red circle;\\n 🟦 is a blue circle;\\n 🔴 is a yellow circle;\\n 🟥 is a red triangle pointing up;\\n 🟨 is a red triangle pointing down;\\n 🔻 is a red square;\\n 🟡 is a blue square;\\n _ is a yellow square;\\n 🔵 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=adversarial,model=neurips_local,max_train_instances=1,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'adversarial'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔺 is a red circle;\\n 🟦 is a blue circle;\\n 🔴 is a yellow circle;\\n 🟥 is a red triangle pointing up;\\n 🟨 is a red triangle pointing down;\\n 🔻 is a red square;\\n 🟡 is a blue square;\\n _ is a yellow square;\\n 🔵 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=adversarial,model=neurips_local,max_train_instances=2,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'adversarial'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔺 is a red circle;\\n 🟦 is a blue circle;\\n 🔴 is a yellow circle;\\n 🟥 is a red triangle pointing up;\\n 🟨 is a red triangle pointing down;\\n 🔻 is a red square;\\n 🟡 is a blue square;\\n _ is a yellow square;\\n 🔵 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=adversarial,model=neurips_local,max_train_instances=3,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'adversarial'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔺 is a red circle;\\n 🟦 is a blue circle;\\n 🔴 is a yellow circle;\\n 🟥 is a red triangle pointing up;\\n 🟨 is a red triangle pointing down;\\n 🔻 is a red square;\\n 🟡 is a blue square;\\n _ is a yellow square;\\n 🔵 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=emoji_agnostic,model=neurips_local,max_train_instances=0,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'emoji_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-emoji-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🐮 is a red circle;\\n 🐺 is a blue circle;\\n 🦁 is a yellow circle;\\n 🐷 is a red triangle pointing up;\\n 🐯 is a red triangle pointing down;\\n 🐱 is a red square;\\n 🦝 is a blue square;\\n 🐔 is a yellow square;\\n 🦄 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=emoji_agnostic,model=neurips_local,max_train_instances=1,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'emoji_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-emoji-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🐮 is a red circle;\\n 🐺 is a blue circle;\\n 🦁 is a yellow circle;\\n 🐷 is a red triangle pointing up;\\n 🐯 is a red triangle pointing down;\\n 🐱 is a red square;\\n 🦝 is a blue square;\\n 🐔 is a yellow square;\\n 🦄 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=emoji_agnostic,model=neurips_local,max_train_instances=2,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'emoji_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-emoji-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🐮 is a red circle;\\n 🐺 is a blue circle;\\n 🦁 is a yellow circle;\\n 🐷 is a red triangle pointing up;\\n 🐯 is a red triangle pointing down;\\n 🐱 is a red square;\\n 🦝 is a blue square;\\n 🐔 is a yellow square;\\n 🦄 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=emoji_agnostic,model=neurips_local,max_train_instances=3,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'emoji_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-emoji-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🐮 is a red circle;\\n 🐺 is a blue circle;\\n 🦁 is a yellow circle;\\n 🐷 is a red triangle pointing up;\\n 🐯 is a red triangle pointing down;\\n 🐱 is a red square;\\n 🦝 is a blue square;\\n 🐔 is a yellow square;\\n 🦄 is an empty space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=3, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=name_agnostic,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'name_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-name-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a X D;\\n 🔵 is a H D;\\n 🟡 is a U D;\\n 🔺 is a X Y G;\\n 🔻 is a X Y A;\\n 🟥 is a X B;\\n 🟦 is a H B;\\n 🟨 is a U B;\\n _ is a T.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=name_agnostic,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'name_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-name-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a X D;\\n 🔵 is a H D;\\n 🟡 is a U D;\\n 🔺 is a X Y G;\\n 🔻 is a X Y A;\\n 🟥 is a X B;\\n 🟦 is a H B;\\n 🟨 is a U B;\\n _ is a T.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=name_agnostic,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'name_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-name-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a X D;\\n 🔵 is a H D;\\n 🟡 is a U D;\\n 🔺 is a X Y G;\\n 🔻 is a X Y A;\\n 🟥 is a X B;\\n 🟦 is a H B;\\n 🟨 is a U B;\\n _ is a T.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=name_agnostic,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'name_agnostic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-name-agnostic world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a X D;\\n 🔵 is a H D;\\n 🟡 is a U D;\\n 🔺 is a X Y G;\\n 🔻 is a X Y A;\\n 🟥 is a X B;\\n 🟦 is a H B;\\n 🟨 is a U B;\\n _ is a T.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=plain,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'plain'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-plain world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a red circle;\\n 🔵 is a blue circle;\\n 🟡 is a yellow circle;\\n 🔺 is a red triangle pointing_up;\\n 🔻 is a red triangle pointing_down;\\n 🟥 is a red square;\\n 🟦 is a blue square;\\n 🟨 is a yellow square;\\n _ is an empty_space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=plain,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'plain'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-plain world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a red circle;\\n 🔵 is a blue circle;\\n 🟡 is a yellow circle;\\n 🔺 is a red triangle pointing_up;\\n 🔻 is a red triangle pointing_down;\\n 🟥 is a red square;\\n 🟦 is a blue square;\\n 🟨 is a yellow square;\\n _ is an empty_space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=plain,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'plain'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-plain world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a red circle;\\n 🔵 is a blue circle;\\n 🟡 is a yellow circle;\\n 🔺 is a red triangle pointing_up;\\n 🔻 is a red triangle pointing_down;\\n 🟥 is a red square;\\n 🟦 is a blue square;\\n 🟨 is a yellow square;\\n _ is an empty_space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=plain,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'plain'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-plain world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a red circle;\\n 🔵 is a blue circle;\\n 🟡 is a yellow circle;\\n 🔺 is a red triangle pointing_up;\\n 🔻 is a red triangle pointing_down;\\n 🟥 is a red square;\\n 🟦 is a blue square;\\n 🟨 is a yellow square;\\n _ is an empty_space.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=tricky,model=neurips_local,max_train_instances=0,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'tricky'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-tricky world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a der elcric;\\n 🔵 is an eulb elcric;\\n 🟡 is a wolley elcric;\\n 🔺 is a der elgnairt gnitniop pu;\\n 🔻 is a der elgnairt gnitniop nwod;\\n 🟥 is a der erauqs;\\n 🟦 is an eulb erauqs;\\n 🟨 is a wolley erauqs;\\n _ is a ytpme ecaps.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=tricky,model=neurips_local,max_train_instances=1,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'tricky'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-tricky world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a der elcric;\\n 🔵 is an eulb elcric;\\n 🟡 is a wolley elcric;\\n 🔺 is a der elgnairt gnitniop pu;\\n 🔻 is a der elgnairt gnitniop nwod;\\n 🟥 is a der erauqs;\\n 🟦 is an eulb erauqs;\\n 🟨 is a wolley erauqs;\\n _ is a ytpme ecaps.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=tricky,model=neurips_local,max_train_instances=2,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'tricky'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-tricky world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a der elcric;\\n 🔵 is an eulb elcric;\\n 🟡 is a wolley elcric;\\n 🔺 is a der elgnairt gnitniop pu;\\n 🔻 is a der elgnairt gnitniop nwod;\\n 🟥 is a der erauqs;\\n 🟦 is an eulb erauqs;\\n 🟨 is a wolley erauqs;\\n _ is a ytpme ecaps.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=symbol_interpretation,subtask=tricky,model=neurips_local,max_train_instances=3,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'symbol_interpretation', 'subtask': 'tricky'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='\\nIn the SIT-tricky world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔴 is a der elcric;\\n 🔵 is an eulb elcric;\\n 🟡 is a wolley elcric;\\n 🔺 is a der elgnairt gnitniop pu;\\n 🔻 is a der elgnairt gnitniop nwod;\\n 🟥 is a der erauqs;\\n 🟦 is an eulb erauqs;\\n 🟨 is a wolley erauqs;\\n _ is a ytpme ecaps.\\n\\n', input_prefix='', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=4, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_symbol_interpretation'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=vitaminc_fact_verification,model=neurips_local,max_train_instances=0,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'vitaminc_fact_verification', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n', input_prefix='\\nPassage: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nTrue, False, or Neither? ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_vitaminc_fact_verification'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=vitaminc_fact_verification,model=neurips_local,max_train_instances=1,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'vitaminc_fact_verification', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n', input_prefix='\\nPassage: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nTrue, False, or Neither? ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_vitaminc_fact_verification'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=vitaminc_fact_verification,model=neurips_local,max_train_instances=2,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'vitaminc_fact_verification', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n', input_prefix='\\nPassage: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nTrue, False, or Neither? ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_vitaminc_fact_verification'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=vitaminc_fact_verification,model=neurips_local,max_train_instances=3,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'vitaminc_fact_verification', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Based only on the information contained in a brief quote from Wikipedia, answer whether the related claim is True, False or Neither. Use Neither when the Wikipedia quote does not provide the necessary information to resolve the question.\\n\\n', input_prefix='\\nPassage: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nTrue, False, or Neither? ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=18, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_vitaminc_fact_verification'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=winowhy,model=neurips_local,max_train_instances=0,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'winowhy', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Please answer the following questions about which words certain pronouns refer to.\\n', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nThe above reasoning is ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=0, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_winowhy'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=winowhy,model=neurips_local,max_train_instances=1,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'winowhy', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Please answer the following questions about which words certain pronouns refer to.\\n', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nThe above reasoning is ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=1, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_winowhy'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=winowhy,model=neurips_local,max_train_instances=2,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'winowhy', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Please answer the following questions about which words certain pronouns refer to.\\n', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nThe above reasoning is ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=2, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_winowhy'], annotators=None)\n",
            "    RunSpec(name='big_bench:task=winowhy,model=neurips_local,max_train_instances=3,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.big_bench_scenario.BIGBenchScenario', args={'task': 'winowhy', 'subtask': ''}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='Please answer the following questions about which words certain pronouns refer to.\\n', input_prefix='\\n', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='\\nThe above reasoning is ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=3, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0, max_tokens=64, stop_sequences=[], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['big_bench_winowhy'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=anatomy,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'anatomy'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about anatomy.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_medicine,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college medicine.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_biology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_biology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college biology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_biology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_biology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school biology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_computer_science,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_computer_science'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college computer science.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_computer_science,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_computer_science'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school computer science.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=computer_security,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'computer_security'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about computer security.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=electrical_engineering,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'electrical_engineering'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about electrical engineering.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=machine_learning,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'machine_learning'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about machine learning.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_mathematics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_mathematics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school mathematics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_mathematics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_mathematics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college mathematics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=abstract_algebra,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'abstract_algebra'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about abstract algebra.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_statistics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_statistics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school statistics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_chemistry,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_chemistry'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college chemistry.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_chemistry,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_chemistry'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school chemistry.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_physics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_physics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school physics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=college_physics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_physics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about college physics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=astronomy,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'astronomy'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about astronomy.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=formal_logic,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'formal_logic'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about formal logic.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=logical_fallacies,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'logical_fallacies'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about logical fallacies.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=philosophy,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'philosophy'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about philosophy.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=moral_disputes,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'moral_disputes'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about moral disputes.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=moral_scenarios,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'moral_scenarios'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about moral scenarios.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=professional_law,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_law'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about professional law.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=international_law,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'international_law'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about international law.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=jurisprudence,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'jurisprudence'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about jurisprudence.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_european_history,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_european_history'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school european history.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_us_history,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_us_history'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school us history.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_world_history,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_world_history'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school world history.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=prehistory,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'prehistory'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about prehistory.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=world_religions,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'world_religions'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about world religions.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=business_ethics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'business_ethics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about business ethics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=global_facts,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'global_facts'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about global facts.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=management,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'management'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about management.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=marketing,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'marketing'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about marketing.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=miscellaneous,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'miscellaneous'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about miscellaneous.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=professional_accounting,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_accounting'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about professional accounting.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=nutrition,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'nutrition'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about nutrition.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=human_aging,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'human_aging'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about human aging.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=clinical_knowledge,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'clinical_knowledge'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about clinical knowledge.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=medical_genetics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'medical_genetics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about medical genetics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=professional_medicine,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_medicine'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about professional medicine.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=virology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'virology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about virology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_government_and_politics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_government_and_politics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school government and politics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_geography,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=3', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_geography'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school geography.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=3, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=us_foreign_policy,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'us_foreign_policy'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about us foreign policy.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=public_relations,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'public_relations'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about public relations.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=security_studies,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'security_studies'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about security studies.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_psychology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_psychology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school psychology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=human_sexuality,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=4', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'human_sexuality'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about human sexuality.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=4, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=professional_psychology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_psychology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about professional psychology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=sociology,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=5', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'sociology'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about sociology.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=5, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_microeconomics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_microeconomics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school microeconomics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=econometrics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'econometrics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about econometrics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='mmlu:subject=high_school_macroeconomics,method=multiple_choice_joint,model=neurips_local,data_augmentation=canonical,max_eval_instances=6', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_macroeconomics'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers) about high school macroeconomics.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=6, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[PerturbationSpec(class_name='helm.benchmark.augmentations.mild_mix_perturbation.MildMixPerturbation', args={}), PerturbationSpec(class_name='helm.benchmark.augmentations.dialect_perturbation.DialectPerturbation', args={'prob': 1.0, 'source_class': 'SAE', 'target_class': 'AAVE', 'mapping_file_path': None}), PerturbationSpec(class_name='helm.benchmark.augmentations.gender_perturbation.GenderPerturbation', args={'mode': 'pronouns', 'prob': 1.0, 'source_class': 'male', 'target_class': 'female', 'mapping_file_path': None, 'mapping_file_genders': None, 'bidirectional': False}), PerturbationSpec(class_name='helm.benchmark.augmentations.person_name_perturbation.PersonNamePerturbation', args={'prob': 1.0, 'source_class': {'race': 'white_american'}, 'target_class': {'race': 'black_american'}, 'name_file_path': None, 'person_name_type': 'first_name', 'preserve_gender': True})], should_augment_train_instances=False, should_include_original_train=True, should_skip_unchanged_train=True, should_augment_eval_instances=True, should_include_original_eval=True, should_skip_unchanged_eval=True, seeds_per_instance=1), groups=['mmlu'], annotators=None)\n",
            "    RunSpec(name='truthful_qa:task=mc_single,method=multiple_choice_joint,model=neurips_local,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=9, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['truthful_qa'], annotators=None)\n",
            "    RunSpec(name='summarization_cnndm:temperature=0.3,device=cpu,model=neurips_local,max_eval_instances=9', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.summarization_scenario.SummarizationScenario', args={'dataset_name': 'cnn-dm', 'sampling_min_length': 50, 'sampling_max_length': 150, 'doc_max_length': 512}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='###\\nArticle: ', input_suffix='\\n\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Summarize the above article in 3 sentences.\\n', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=9, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.3, max_tokens=128, stop_sequences=['###'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.summarization_metrics.SummarizationMetric', args={'task': 'summarization_cnndm', 'device': 'cpu'}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': []}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['summarization_cnndm'], annotators=None)\n",
            "    RunSpec(name='gsm:model=neurips_local,max_eval_instances=19', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.gsm_scenario.GSM8KScenario', args={}), adapter_spec=AdapterSpec(method='generation', global_prefix='', global_suffix='', instructions='', input_prefix='Q: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='A: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=19, num_outputs=1, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=400, stop_sequences=['\\n\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match_indicator', 'final_number_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['gsm'], annotators=None)\n",
            "    RunSpec(name='bbq:subject=all,method=multiple_choice_joint,model=neurips_local,max_eval_instances=18', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.bbq_scenario.BBQScenario', args={'subject': 'all'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', global_suffix='', instructions='The following are multiple choice questions (with answers).\\n', input_prefix='Passage: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=18, num_outputs=5, num_train_trials=1, num_trials=1, sample_train=True, model_deployment='neurips/local', model='neurips/local', temperature=0.0, max_tokens=1, stop_sequences=['\\n'], random=None, multi_label=False, image_generation_parameters=None, eval_splits=None), metric_specs=[MetricSpec(class_name='helm.benchmark.metrics.bbq_metrics.BBQMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicGenerationMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.BasicReferenceMetric', args={}), MetricSpec(class_name='helm.benchmark.metrics.basic_metrics.InstancesPerSplitMetric', args={})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['bbq'], annotators=None)\n",
            "  } [0.597s]\n",
            "  Running in local mode with base path: prod_env\n",
            "Looking in path: prod_env\n",
            "  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')\n",
            "  AutoClient: file_storage_path = prod_env/cache\n",
            "  AutoClient: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')\n",
            "  AutoTokenizer: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')\n",
            "  There were no accounts. Created an admin account with API key: root\n",
            "Looking in path: prod_env\n",
            "  AnnotatorFactory: file_storage_path = prod_env/cache\n",
            "  AnnotatorFactory: cache_backend_config = SqliteCacheBackendConfig(path='prod_env/cache')\n",
            "  0% 0/175 [00:00<?, ?it/s]  Running big_bench:task=auto_debugging,model=neurips_local,max_train_instances=0,max_eval_instances=18 {\n",
            "    scenario.get_instances {\n",
            "      ensure_file_downloaded {\n",
            "        Not downloading https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/auto_debugging/task.json because benchmark_output/scenarios/big_bench/auto_debugging/task.json already exists\n",
            "      } [0.0s]\n",
            "    } [0.001s]\n",
            "    34 instances, 2 train instances, 18/32 eval instances\n",
            "    DataPreprocessor.preprocess {\n",
            "    } [0.0s]\n",
            "    GenerationAdapter.adapt {\n",
            "      20 instances, choosing 0/2 train instances, 18 eval instances\n",
            "      Adapting with train_trial_index=0 {\n",
            "        Sampled 0 examples for trial #0.\n",
            "        Parallelizing computation on 18 items over 4 threads {\n",
            "\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Failed to tokenize after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Failed to tokenize after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Failed to tokenize after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Failed to tokenize after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Failed to decode after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Failed to decode after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Failed to decode after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/decode\n",
            "          Failed to decode after retrying 5 times\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #2) in 2 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #3) in 4 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #4) in 8 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "          Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "          Request failed. Retrying (attempt #5) in 16 seconds... (See above for error details)\n",
            "  0% 0/18 [01:23<?, ?it/s]\n",
            "        } [1m23.457s]\n",
            "      } [1m23.457s]\n",
            "    } [1m23.457s]\n",
            "  } [1m23.469s]\n",
            "  0% 0/175 [01:23<?, ?it/s]\n",
            "} [1m32.556s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/common/general.py\", line 235, in parallel_map\n",
            "    results = list(tqdm(executor.map(process, items), total=len(items), disable=None))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n",
            "    yield _result_or_cancel(fs.pop())\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n",
            "    return fut.result(timeout)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 453, in result\n",
            "    self._condition.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 320, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/helm-run\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/common/hierarchical_logger.py\", line 104, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/run.py\", line 321, in main\n",
            "    run_benchmarking(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/run.py\", line 125, in run_benchmarking\n",
            "    runner.run_all(run_specs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/runner.py\", line 216, in run_all\n",
            "    self.run_one(run_spec)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/runner.py\", line 283, in run_one\n",
            "    request_states: List[RequestState] = adapter.adapt(instances, self.executor.execution_spec.parallelism)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/common/hierarchical_logger.py\", line 104, in wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/adaptation/adapters/in_context_learning_adapter.py\", line 63, in adapt\n",
            "    self._adapt_trial_index(all_train_instances, train_trial_index, eval_instances, parallelism)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/benchmark/adaptation/adapters/in_context_learning_adapter.py\", line 86, in _adapt_trial_index\n",
            "    results: List[List[RequestState]] = parallel_map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/helm/common/general.py\", line 234, in parallel_map\n",
            "    with ThreadPoolExecutor(max_workers=parallelism) as executor:\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 649, in __exit__\n",
            "    self.shutdown(wait=True)\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 235, in shutdown\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "Failed to tokenize after retrying 5 times\n",
            "Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "Failed to tokenize after retrying 5 times\n",
            "Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "Failed to tokenize after retrying 5 times\n",
            "Local Model error: 404 Client Error: Not Found for url: http://localhost:8080/tokenize\n",
            "Failed to tokenize after retrying 5 times\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! helm-summarize --suite v1\n"
      ],
      "metadata": {
        "id": "BY4BZBUTnZhf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}