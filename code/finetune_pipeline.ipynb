{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q -U bitsandbytes==0.42.0\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up packages\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import logging\n",
    "import json \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from google.colab import userdata\n",
    "\n",
    "# Huggingface token\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Setup device \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s',\n",
    "                     datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "                     filename='logfile.txt',\n",
    "                     level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeLanguageModel():\n",
    "    def __init__(self,model_id,bnb_config = False):\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # hyperparameters \n",
    "        with open('params.json', 'r') as file:\n",
    "            params = json.load(file)\n",
    "        self.train_params =params[\"training\"]\n",
    "        lora_params =params[\"lora\"]\n",
    "        self.token = os.environ['HF_TOKEN']\n",
    "        self.bnb_config = bnb_config\n",
    "        \n",
    "        # Model object\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id,\n",
    "                                             quantization_config=self.bnb_config,\n",
    "                                             device_map={\"\":0},\n",
    "                                             token=self.token)\n",
    "        # Tokenizer object\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, token=self.token)\n",
    "        \n",
    "        # Lora object\n",
    "        self.lora_config = LoraConfig(\n",
    "                                    r=lora_params[\"r\"],\n",
    "                                    target_modules=lora_params[\"target_modules\"],\n",
    "                                    task_type=lora_params[\"task_type\"],\n",
    "                                )\n",
    "        \n",
    "        logging.info(\"LLM class instantiated\")\n",
    "        logging.info(f\"Hyperparameters: \\n{params}\")\n",
    "        \n",
    "    \n",
    "    def generate_example(self,text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=50)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def formatting_func(example):\n",
    "        text = f\"<start_of_turn>user\\n{example['INSTRUCTION'][0]}<end_of_turn> <start_of_turn>model\\n{example['RESPONSE'][0]}<end_of_turn>\"\n",
    "        return [text]\n",
    "\n",
    "    def finetune(self,dataset):\n",
    "        logging.info(\"Finetuning started\")\n",
    "        trainer = SFTTrainer(\n",
    "            model = self.model,\n",
    "            train_dataset = dataset.data[\"train\"],\n",
    "            args=transformers.TrainingArguments(\n",
    "                per_device_train_batch_size = self.train_params[\"per_device_train_batch_size\"],\n",
    "                gradient_accumulation_steps = self.train_params[\"gradient_accumulation_steps\"],\n",
    "                warmup_steps = self.train_params[\"warmup_steps\"],\n",
    "                max_steps = self.train_params[\"max_steps\"],\n",
    "                learning_rate = self.train_params[\"learning_rate\"],\n",
    "                fp16 = self.train_params[\"fp16\"],\n",
    "                logging_steps = self.train_params[\"logging_steps\"],\n",
    "                output_dir = self.train_params[\"output_dir\"],\n",
    "                optim = self.train_params[\"optim\"]\n",
    "            ),\n",
    "            peft_config = self.lora_config,\n",
    "            formatting_func = self.formatting_func,\n",
    "        )\n",
    "        trainer.train()\n",
    "        logging.info(\"Finetuning completed\")\n",
    "\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self, dataset_id):\n",
    "        logging.info(\"Loading dataset\")\n",
    "        self.data = load_dataset(dataset_id)\n",
    "        logging.info(\"Loading dataset completed\")\n",
    "\n",
    "\n",
    "    def print_dataset_values(self):\n",
    "        print(self.data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model and dataset\n",
    "model_id = \"google/gemma-2b\"\n",
    "dataset_id = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "# Setting up Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Generating objects \n",
    "llm = LargeLanguageModel(model_id, bnb_config)\n",
    "dataset = Dataset()\n",
    "dataset.load_data(dataset_id)\n",
    "dataset.return_dataset_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning\n",
    "llm.finetune(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
